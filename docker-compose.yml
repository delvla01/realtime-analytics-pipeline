
version: '3.8'

x-airflow-common: &airflow-common
  build: .
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:
    AIRFLOW__WEBSERVER__SECRET_KEY: ''
  volumes:
    - ./orchestration/dags:/opt/airflow/dags
    - .:/opt/airflow/realtime-analytics-pipeline
  user: "${AIRFLOW_UID:-50000}:0"

services:
  zookeeper:
    image: wurstmeister/zookeeper
    platform: linux/amd64
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka
    platform: linux/amd64
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    depends_on:
      - zookeeper

  postgres:
    image: postgres:13
    container_name: postgres
    platform: linux/arm64
    environment:
      POSTGRES_USER:
      POSTGRES_PASSWORD:
      POSTGRES_DB: airflow_metadata
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data

  spark:
    image: bitnami/spark:3.3.2
    container_name: spark
    platform: linux/amd64
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - .:/opt/bitnami/spark/project
    depends_on:
      - kafka
      - postgres
    command: bash -c "tail -f /dev/null"

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    depends_on:
      - airflow-scheduler

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      - postgres
      - kafka

  airflow-init:
    <<: *airflow-common
    depends_on:
      - postgres
    command: >
      bash -c "
        sleep 10 &&
        airflow db migrate &&
        airflow users create --username airflow --password airflow --firstname Air --lastname Flow --role Admin --email airflow@example.com
      "

  event-simulator:
    build:
      context: .
      dockerfile: Dockerfile.producer 
    container_name: event-simulator
    depends_on:
      - kafka
    restart: always

volumes:
  pg_data:
