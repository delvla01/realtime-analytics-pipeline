# realtime-analytics-pipeline
---

## âš™ï¸ Tech Stack

| Layer         | Tool/Service                     |
|---------------|----------------------------------|
| Data Source   | Python script simulating events |
| Messaging     | Apache Kafka + Zookeeper (Docker) |
| Stream Processor | Apache Spark (PySpark)       |
| Storage       | PostgreSQL (Docker)              |
| Orchestration | Manual / Optional: Apache Airflow |
| Analytics     | SQL, dbt (optional), Streamlit   |

---

## ğŸ’¡ Key Features

- Simulates live user behavior (click, scroll, purchase)
- Ingests real-time data using Kafka
- Processes data with Spark Structured Streaming
- Writes micro-batches to PostgreSQL using `foreachBatch`
- Schema-first modeling and real-time querying
- Fully local and free â€” ideal for learning or showcasing data engineering skills

---

## ğŸ“‚ Project Structure

```bash
realtime-analytics-pipeline/
â”‚
â”œâ”€â”€ simulation/               # Simulated event generator (Python)
â”‚   â””â”€â”€ event_producer.py
â”‚
â”œâ”€â”€ streaming/                # PySpark consumer + processor
â”‚   â””â”€â”€ process_stream.py
â”‚
â”œâ”€â”€ dashboard/                # Streamlit app for real-time viz (optional)
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ warehouse/                # dbt project for modeling (optional)
â”‚
â”œâ”€â”€ configs/                  # Environment/sample config
â”‚
â”œâ”€â”€ docker-compose.yml        # Kafka, Zookeeper, PostgreSQL
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # This file
